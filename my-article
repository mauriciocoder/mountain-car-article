#### Titles

Empirical convergence analysis of Radial Basis Functions and polynomial feature selections on Episodic Semi-gradient Sarsa Td(0)



#### Keywords:
Polynomials
Radial Basis Functions
Episodic Semi-gradient Sarsa Td(0) for Estimating q̂
Empirical


#### Questions:

How to compare convergence in reinforcement learning algorithms?!




#### Excerts


This excert explains why the TD is semi-gradient

This step would not be valid if a bootstrapping estimate were used in place of v⇡ (St ). Bootstrapping methods are not
in fact instances of true gradient descent (Barnard, 1993). They take into account the e↵ect of changing the weight vector wt on the estimate, but ignore its e↵ect on the target.
They include only a part of the gradient and, accordingly, we call them semi-gradient
methods.
Although semi-gradient (bootstrapping) methods do not converge as robustly as
gradient methods, they do converge reliably in important cases such as the linear case
discussed in the next section. Moreover, they o↵er important advantages that make them
often clearly preferred. One reason for this is that they typically enable significantly faster
learning, as we have seen in Chapters 6 and 7. Another is that they enable learning to9.3. Stochastic-gradient and Semi-gradient Methods
203
be continual and online, without waiting for the end of an episode. This enables them to
be used on continuing problems and provides computational advantages. A prototypical
.
semi-gradient method is semi-gradient TD(0), which uses Ut = Rt+1 + v̂(St+1 ,w) as its
target. Complete pseudocode for this method is given in the box below.




We have applied a linear method, and this explains why

Because it is so simple, the linear SGD case is one of the most favorable for mathematical
analysis. Almost all useful convergence results for learning systems of all kinds are for
linear (or simpler) function approximation methods.
In particular, in the linear case there is only one optimum (or, in degenerate cases,
one set of equally good optima), and thus any method that is guaranteed to converge to
or near a local optimum is automatically guaranteed to converge to or near the global
optimum.



Polynomials

Polynomials make up one of the simplest families of
features used for interpolation and regression. While the basic polynomial features we
discuss here do not work as well as other types of features in reinforcement learning, they
serve as a good introduction because they are simple and familiar.

Please check page 233 from book, there we can see the real polynomial-basis feature definition



Radial Basis Functions

Definition is extracted from page 243 of the book

Returns the Normalized Basis Vector (Gaussian) for a given state.
        Reference: https://en.wikipedia.org/wiki/Radial_basis_function








### Articles:

Barnard, E. (1993). Temporal-diference methods and Markov models. IEEE Transactions on
Systems, Man, and Cybernetics, 23(2):357–365.

In appendix 1 - the author proves that TD is not a gradient-descent method, so we call ir semi-gradient methods






### Text instalation:

https://www.tug.org/texlive/quickinstall.html
